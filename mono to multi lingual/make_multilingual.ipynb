{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in f:\\quran-indo\\.venv\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in f:\\quran-indo\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in f:\\quran-indo\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in f:\\quran-indo\\.venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in f:\\quran-indo\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.14-cp312-cp312-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in f:\\quran-indo\\.venv\\lib\\site-packages (from datasets) (0.29.2)\n",
      "Requirement already satisfied: packaging in f:\\quran-indo\\.venv\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in f:\\quran-indo\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.2.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.0-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp312-cp312-win_amd64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in f:\\quran-indo\\.venv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\quran-indo\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\quran-indo\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\quran-indo\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\quran-indo\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in f:\\quran-indo\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in f:\\quran-indo\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in f:\\quran-indo\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in f:\\quran-indo\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in f:\\quran-indo\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading aiohttp-3.11.14-cp312-cp312-win_amd64.whl (438 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pyarrow-19.0.1-cp312-cp312-win_amd64.whl (25.3 MB)\n",
      "   ---------------------------------------- 0.0/25.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/25.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/25.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/25.3 MB 882.6 kB/s eta 0:00:29\n",
      "    --------------------------------------- 0.5/25.3 MB 882.6 kB/s eta 0:00:29\n",
      "   - -------------------------------------- 0.8/25.3 MB 907.1 kB/s eta 0:00:27\n",
      "   - -------------------------------------- 1.0/25.3 MB 914.5 kB/s eta 0:00:27\n",
      "   -- ------------------------------------- 1.3/25.3 MB 944.7 kB/s eta 0:00:26\n",
      "   -- ------------------------------------- 1.3/25.3 MB 944.7 kB/s eta 0:00:26\n",
      "   -- ------------------------------------- 1.6/25.3 MB 975.2 kB/s eta 0:00:25\n",
      "   -- ------------------------------------- 1.8/25.3 MB 1.0 MB/s eta 0:00:24\n",
      "   --- ------------------------------------ 2.1/25.3 MB 1.0 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 2.4/25.3 MB 1.0 MB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 2.6/25.3 MB 1.0 MB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 2.9/25.3 MB 1.1 MB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 3.1/25.3 MB 1.1 MB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 3.4/25.3 MB 1.1 MB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 3.7/25.3 MB 1.1 MB/s eta 0:00:20\n",
      "   ------ --------------------------------- 3.9/25.3 MB 1.1 MB/s eta 0:00:20\n",
      "   ------ --------------------------------- 4.2/25.3 MB 1.1 MB/s eta 0:00:19\n",
      "   ------- -------------------------------- 4.5/25.3 MB 1.1 MB/s eta 0:00:19\n",
      "   ------- -------------------------------- 4.7/25.3 MB 1.1 MB/s eta 0:00:19\n",
      "   ------- -------------------------------- 5.0/25.3 MB 1.1 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 5.2/25.3 MB 1.1 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 5.5/25.3 MB 1.1 MB/s eta 0:00:18\n",
      "   --------- ------------------------------ 5.8/25.3 MB 1.2 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 6.0/25.3 MB 1.2 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 6.3/25.3 MB 1.2 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 6.6/25.3 MB 1.2 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 6.8/25.3 MB 1.2 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 7.1/25.3 MB 1.2 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 7.3/25.3 MB 1.2 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 7.6/25.3 MB 1.2 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 7.9/25.3 MB 1.2 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 7.9/25.3 MB 1.2 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 8.1/25.3 MB 1.2 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 8.4/25.3 MB 1.2 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 8.7/25.3 MB 1.2 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 8.9/25.3 MB 1.2 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 9.2/25.3 MB 1.2 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 9.4/25.3 MB 1.2 MB/s eta 0:00:14\n",
      "   --------------- ------------------------ 9.7/25.3 MB 1.2 MB/s eta 0:00:14\n",
      "   --------------- ------------------------ 10.0/25.3 MB 1.2 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 10.2/25.3 MB 1.2 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 10.5/25.3 MB 1.2 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 10.7/25.3 MB 1.2 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 11.0/25.3 MB 1.2 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 11.3/25.3 MB 1.2 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 11.5/25.3 MB 1.2 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 11.8/25.3 MB 1.2 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 12.1/25.3 MB 1.2 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 12.3/25.3 MB 1.2 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 12.6/25.3 MB 1.2 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 12.8/25.3 MB 1.2 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 13.1/25.3 MB 1.2 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 13.1/25.3 MB 1.2 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 13.4/25.3 MB 1.2 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 13.6/25.3 MB 1.2 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 13.9/25.3 MB 1.2 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 14.2/25.3 MB 1.2 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 14.4/25.3 MB 1.2 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 14.7/25.3 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 14.9/25.3 MB 1.2 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 14.9/25.3 MB 1.2 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 15.2/25.3 MB 1.2 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 15.5/25.3 MB 1.2 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 15.7/25.3 MB 1.2 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 16.0/25.3 MB 1.2 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 16.0/25.3 MB 1.2 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 16.3/25.3 MB 1.2 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 16.5/25.3 MB 1.2 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 16.8/25.3 MB 1.2 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 16.8/25.3 MB 1.2 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 17.0/25.3 MB 1.2 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 17.3/25.3 MB 1.2 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 17.6/25.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 17.8/25.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 17.8/25.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 18.1/25.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 18.4/25.3 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 18.6/25.3 MB 1.1 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 18.9/25.3 MB 1.1 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 18.9/25.3 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 19.1/25.3 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 19.4/25.3 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 19.7/25.3 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 19.7/25.3 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 19.9/25.3 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 20.2/25.3 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 20.4/25.3 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 20.4/25.3 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 20.7/25.3 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 21.0/25.3 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 21.2/25.3 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 21.5/25.3 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 21.5/25.3 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 21.8/25.3 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 22.0/25.3 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 22.3/25.3 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 22.3/25.3 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 22.5/25.3 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 22.8/25.3 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 22.8/25.3 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 23.1/25.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 23.1/25.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 23.3/25.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 23.3/25.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 23.6/25.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 23.9/25.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 23.9/25.3 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 24.1/25.3 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 24.4/25.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/25.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.3/25.3 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-win_amd64.whl (51 kB)\n",
      "Downloading multidict-6.2.0-cp312-cp312-win_amd64.whl (28 kB)\n",
      "Downloading propcache-0.3.0-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Downloading yarl-1.18.3-cp312-cp312-win_amd64.whl (90 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 attrs-25.3.0 datasets-3.4.1 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 multidict-6.2.0 multiprocess-0.70.16 propcache-0.3.0 pyarrow-19.0.1 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script contains an example how to extend an existent sentence embedding model to new languages.\n",
    "\n",
    "Given a (monolingual) teacher model you would like to extend to new languages, which is specified in the teacher_model_name\n",
    "variable. We train a multilingual student model to imitate the teacher model (variable student_model_name)\n",
    "on multiple languages.\n",
    "\n",
    "For training, you need parallel sentence data (machine translation training data). You need tab-seperated files (.tsv)\n",
    "with the first column a sentence in a language understood by the teacher model, e.g. English,\n",
    "and the further columns contain the according translations for languages you want to extend to.\n",
    "\n",
    "This scripts downloads automatically the parallel sentences corpus. This corpus contains transcripts from\n",
    "talks translated to 100+ languages. For other parallel data, see get_parallel_data_[].py scripts\n",
    "\n",
    "Further information can be found in our paper:\n",
    "Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation\n",
    "https://arxiv.org/abs/2004.09813\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "from sentence_transformers import LoggingHandler, SentenceTransformer\n",
    "from sentence_transformers.evaluation import (\n",
    "    EmbeddingSimilarityEvaluator,\n",
    "    MSEEvaluator,\n",
    "    SequentialEvaluator,\n",
    "    TranslationEvaluator,\n",
    ")\n",
    "from sentence_transformers.losses import MSELoss\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO, handlers=[LoggingHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# The teacher model is monolingual, we use it for English embeddings\n",
    "teacher_model_name = \"paraphrase-distilroberta-base-v2\"\n",
    "# The student model is multilingual, we train it such that embeddings of non-English texts mimic the teacher model's English embeddings\n",
    "student_model_name = \"xlm-roberta-base\"\n",
    "\n",
    "student_max_seq_length = 128  # Student model max. lengths for inputs (number of word pieces)\n",
    "train_batch_size = 64  # Batch size for training\n",
    "inference_batch_size = 64  # Batch size at inference\n",
    "max_sentences_per_language = 500000  # Maximum number of  parallel sentences for training\n",
    "\n",
    "num_train_epochs = 5  # Train for x epochs\n",
    "num_evaluation_steps = 5000  # Evaluate performance after every xxxx steps\n",
    "\n",
    "\n",
    "# Define the language codes you would like to extend the model to\n",
    "source_languages = set([\"en\"])  # Our teacher model accepts English (en) sentences\n",
    "# We want to extend the model to these new languages. For language codes, see the header of the train file\n",
    "target_languages = set([\"id\"])\n",
    "\n",
    "\n",
    "output_dir = (\n",
    "    \"output/make-multilingual-\"\n",
    "    + \"-\".join(sorted(list(source_languages)) + sorted(list(target_languages)))\n",
    "    + \"-\"\n",
    "    + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-19 05:33:50 - Use pytorch device_name: cpu\n",
      "2025-03-19 05:33:50 - Load pretrained SentenceTransformer: paraphrase-distilroberta-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76dc6e4d9a04ed2ac75b27eb4c8e982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfcd70e8cf2b46199e20f3901787b555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7936441d7fcc4dab9147fd5473bd2489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69a9a6ed97b45549129e5b663381a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c82fa5d06943e8be4f81b00568777f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/686 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc81ce3920c341f68b43a93e3dc0dd37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21916921b0114eb0bec8af4ab3fe7c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6676bc0fe48404b8afb4ca6f979d3a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce286409a28f45dda47ba030339617ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761f1a37480441e9914ba12552da6bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7a447ed3d94416bc6800310fcc0cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824c50ed33ce486ebb2c00a74b63f3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-19 05:38:38 - Teacher model: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "2025-03-19 05:38:38 - Use pytorch device_name: cpu\n",
      "2025-03-19 05:38:38 - Load pretrained SentenceTransformer: xlm-roberta-base\n",
      "2025-03-19 05:38:39 - No sentence-transformers model found with name xlm-roberta-base. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0450fb226784d4bba5a6154468ddbc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb27e2f518c641a592ead35617ee2679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1a. Here we define our SentenceTransformer teacher model.\n",
    "teacher_model = SentenceTransformer(teacher_model_name)\n",
    "# If we want, we can limit the maximum sequence length for the model\n",
    "# teacher_model.max_seq_length = 128\n",
    "logging.info(f\"Teacher model: {teacher_model}\")\n",
    "\n",
    "# 1b. Here we define our SentenceTransformer student model. If not already a Sentence Transformer model,\n",
    "# it will automatically create one with \"mean\" pooling.\n",
    "student_model = SentenceTransformer(student_model_name)\n",
    "# If we want, we can limit the maximum sequence length for the model\n",
    "student_model.max_seq_length = student_max_seq_length\n",
    "logging.info(f\"Student model: {student_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the parallel sentences training dataset: https://huggingface.co/datasets?other=sentence-transformers&sort=trending&search=parallel-sentences\n",
    "# NOTE: We can also use multiple datasets if we want\n",
    "dataset_to_use = \"sentence-transformers/parallel-sentences-talks\"\n",
    "# dataset_to_use = \"sentence-transformers/parallel-sentences-europarl\"\n",
    "# dataset_to_use = \"sentence-transformers/parallel-sentences-global-voices\"\n",
    "# dataset_to_use = \"sentence-transformers/parallel-sentences-muse\"\n",
    "# dataset_to_use = \"sentence-transformers/parallel-sentences-jw300\"\n",
    "# dataset_to_use = \"sentence-transformers/parallel-sentences-news-commentary\"\n",
    "# dataset_to_use = \"sentence-transformers/parallel-sentences-opensubtitles\"\n",
    "# dataset_to_use = \"sentence-transformers/parallel-sentences-tatoeba\"\n",
    "# dataset_to_use = \"sentence-transformers/parallel-sentences-wikimatrix\"\n",
    "# dataset_to_use = \"sentence-transformers/parallel-sentences-wikititles\"\n",
    "train_dataset_dict = DatasetDict()\n",
    "eval_dataset_dict = DatasetDict()\n",
    "for source_lang in source_languages:\n",
    "    for target_lang in target_languages:\n",
    "        subset = f\"{source_lang}-{target_lang}\"\n",
    "        try:\n",
    "            train_dataset = load_dataset(dataset_to_use, subset, split=\"train\")\n",
    "            if len(train_dataset) > max_sentences_per_language:\n",
    "                train_dataset = train_dataset.select(range(max_sentences_per_language))\n",
    "        except Exception as exc:\n",
    "            logging.error(f\"Could not load dataset {dataset_to_use}/{source_lang}-{target_lang}: {exc}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            eval_dataset = load_dataset(dataset_to_use, subset, split=\"dev\")\n",
    "            if len(eval_dataset) > 1000:\n",
    "                eval_dataset = eval_dataset.select(range(1000))\n",
    "        except Exception:\n",
    "            logging.info(\n",
    "                f\"Could not load dataset {dataset_to_use}/{source_lang}-{target_lang} dev split, splitting 1k samples from train\"\n",
    "            )\n",
    "            dataset = train_dataset.train_test_split(test_size=1000, shuffle=True)\n",
    "            train_dataset = dataset[\"train\"]\n",
    "            eval_dataset = dataset[\"test\"]\n",
    "\n",
    "        train_dataset_dict[subset] = train_dataset\n",
    "        eval_dataset_dict[subset] = eval_dataset\n",
    "logging.info(train_dataset_dict)\n",
    "\n",
    "\n",
    "# We want the student EN embeddings to be similar to the teacher EN embeddings and\n",
    "# the student non-EN embeddings to be similar to the teacher EN embeddings\n",
    "def prepare_dataset(batch):\n",
    "    return {\n",
    "        \"english\": batch[\"english\"],\n",
    "        \"non_english\": batch[\"non_english\"],\n",
    "        \"label\": teacher_model.encode(batch[\"english\"], batch_size=inference_batch_size, show_progress_bar=False),\n",
    "    }\n",
    "\n",
    "\n",
    "column_names = list(train_dataset_dict.values())[0].column_names\n",
    "train_dataset_dict = train_dataset_dict.map(\n",
    "    prepare_dataset, batched=True, batch_size=30000, remove_columns=column_names\n",
    ")\n",
    "logging.info(\"Prepared datasets for training:\", train_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 3. Define our training loss\n",
    "# MSELoss (https://sbert.net/docs/package_reference/sentence_transformer/losses.html#mseloss) needs one text columns and one\n",
    "# column with embeddings from the teacher model\n",
    "train_loss = MSELoss(model=student_model)\n",
    "\n",
    "# 4. Define evaluators for use during training. This is useful to keep track of alongside the evaluation loss.\n",
    "evaluators = []\n",
    "\n",
    "for subset, eval_dataset in eval_dataset_dict.items():\n",
    "    logger.info(f\"Creating evaluators for {subset}\")\n",
    "\n",
    "    # Mean Squared Error (MSE) measures the (euclidean) distance between teacher and student embeddings\n",
    "    dev_mse = MSEEvaluator(\n",
    "        source_sentences=eval_dataset[\"english\"],\n",
    "        target_sentences=eval_dataset[\"non_english\"],\n",
    "        name=subset,\n",
    "        teacher_model=teacher_model,\n",
    "        batch_size=inference_batch_size,\n",
    "    )\n",
    "    evaluators.append(dev_mse)\n",
    "\n",
    "    # TranslationEvaluator computes the embeddings for all parallel sentences. It then check if the embedding of\n",
    "    # source[i] is the closest to target[i] out of all available target sentences\n",
    "    dev_trans_acc = TranslationEvaluator(\n",
    "        source_sentences=eval_dataset[\"english\"],\n",
    "        target_sentences=eval_dataset[\"non_english\"],\n",
    "        name=subset,\n",
    "        batch_size=inference_batch_size,\n",
    "    )\n",
    "    evaluators.append(dev_trans_acc)\n",
    "\n",
    "    # Try to load this subset from STS17\n",
    "    test_dataset = None\n",
    "    try:\n",
    "        test_dataset = load_dataset(\"mteb/sts17-crosslingual-sts\", subset, split=\"test\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            test_dataset = load_dataset(\"mteb/sts17-crosslingual-sts\", f\"{subset[3:]}-{subset[:2]}\", split=\"test\")\n",
    "            subset = f\"{subset[3:]}-{subset[:2]}\"\n",
    "        except Exception:\n",
    "            pass\n",
    "    if test_dataset:\n",
    "        test_evaluator = EmbeddingSimilarityEvaluator(\n",
    "            sentences1=test_dataset[\"sentence1\"],\n",
    "            sentences2=test_dataset[\"sentence2\"],\n",
    "            scores=[score / 5.0 for score in test_dataset[\"score\"]],  # Convert 0-5 scores to 0-1 scores\n",
    "            batch_size=inference_batch_size,\n",
    "            name=f\"sts17-{subset}-test\",\n",
    "            show_progress_bar=False,\n",
    "        )\n",
    "        evaluators.append(test_evaluator)\n",
    "\n",
    "evaluator = SequentialEvaluator(evaluators, main_score_function=lambda scores: np.mean(scores))\n",
    "# Now also prepare the evaluation datasets for training\n",
    "eval_dataset_dict = eval_dataset_dict.map(prepare_dataset, batched=True, batch_size=30000, remove_columns=column_names)\n",
    "\n",
    "# 5. Define the training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=output_dir,\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=train_batch_size,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "    learning_rate=2e-5,\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=num_evaluation_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=num_evaluation_steps,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    run_name=f\"multilingual-{'-'.join(source_languages)}-{'-'.join(target_languages)}\",  # Will be used in W&B if `wandb` is installed\n",
    ")\n",
    "\n",
    "# 6. Create the trainer & start training\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=student_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset_dict,\n",
    "    eval_dataset=eval_dataset_dict,\n",
    "    loss=train_loss,\n",
    "    evaluator=evaluator,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# 7. Save the trained & evaluated model locally\n",
    "final_output_dir = f\"{output_dir}/final\"\n",
    "student_model.save(final_output_dir)\n",
    "\n",
    "# 8. (Optional) save the model to the Hugging Face Hub!\n",
    "# It is recommended to run `huggingface-cli login` to log into your Hugging Face account first\n",
    "model_name = student_model_name if \"/\" not in student_model_name else student_model_name.split(\"/\")[-1]\n",
    "try:\n",
    "    student_model.push_to_hub(f\"{model_name}-multilingual-{'-'.join(source_languages)}-{'-'.join(target_languages)}\")\n",
    "except Exception:\n",
    "    logging.error(\n",
    "        f\"Error uploading model to the Hugging Face Hub:\\n{traceback.format_exc()}To upload it manually, you can run \"\n",
    "        f\"`huggingface-cli login`, followed by loading the model using `model = SentenceTransformer({final_output_dir!r})` \"\n",
    "        f\"and saving it using `model.push_to_hub('{model_name}-multilingual-{'-'.join(source_languages)}-{'-'.join(target_languages)}')`.\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
