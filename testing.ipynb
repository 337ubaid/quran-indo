{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ubaid\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get embedding from BERT with visible tokenization output\n",
    "def get_embedding(text, tokenizer, model):\n",
    "    # Tokenizing the input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    \n",
    "    # Print the tokenized components\n",
    "    tokens = tokenizer.tokenize(text)  # Get tokens as text\n",
    "    token_ids = inputs['input_ids'][0].numpy()  # Convert tensor to numpy array for readability\n",
    "    \n",
    "    # Display tokenization details\n",
    "    print(f\"Original Text: {text}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    print(f\"Attention Mask: {inputs['attention_mask'][0].numpy()}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Pass through model without computing gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Return the embedding from [CLS] token representation\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query and Ayat\n",
    "query = \"adil\"\n",
    "ayat_list = [\"Sesungguhnya Allah menyuruh kamu berlaku adil dan berbuat kebajikan.\", \n",
    "             \"Janganlah sekali-kali kebencianmu terhadap suatu kaum mendorong kamu untuk berlaku tidak adil.\",\n",
    "             \"Allah tidak menyukai orang yang berbuat zalim.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"indolem/indobert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: adil\n",
      "Tokens: ['adil']\n",
      "Token IDs: [   3 6187    4]\n",
      "Attention Mask: [1 1 1]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Text: Sesungguhnya Allah menyuruh kamu berlaku adil dan berbuat kebajikan.\n",
      "Tokens: ['sesungguhnya', 'allah', 'menyuruh', 'kamu', 'berlaku', 'adil', 'dan', 'berbuat', 'kebajikan', '.']\n",
      "Token IDs: [    3  4347  2211  9559  3162  3878  6187  1501  5970 13528    18     4]\n",
      "Attention Mask: [1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Text: Janganlah sekali-kali kebencianmu terhadap suatu kaum mendorong kamu untuk berlaku tidak adil.\n",
      "Tokens: ['janganlah', 'sekali', '-', 'kali', 'kebencian', '##mu', 'terhadap', 'suatu', 'kaum', 'mendorong', 'kamu', 'untuk', 'berlaku', 'tidak', 'adil', '.']\n",
      "Token IDs: [    3 16420  2268    17  1994 12043  2250  1973  2170  2800  5026  3162\n",
      "  1559  3878  1580  6187    18     4]\n",
      "Attention Mask: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Text: Allah tidak menyukai orang yang berbuat zalim.\n",
      "Tokens: ['allah', 'tidak', 'menyukai', 'orang', 'yang', 'berbuat', 'zalim', '.']\n",
      "Token IDs: [    3  2211  1580  8127  1646  1497  5970 20756    18     4]\n",
      "Attention Mask: [1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get embeddings for query and ayat\n",
    "query_embedding = get_embedding(query, tokenizer, model)\n",
    "ayat_embeddings = [get_embedding(ayat, tokenizer, model) for ayat in ayat_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: adil\n",
      "Tokens: ['ad', '##il']\n",
      "Token IDs: [  101 10840 11030   102]\n",
      "Attention Mask: [1 1 1 1]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Text: Sesungguhnya Allah menyuruh kamu berlaku adil dan berbuat kebajikan.\n",
      "Tokens: ['Ses', '##ung', '##gu', '##hnya', 'Allah', 'men', '##yu', '##ruh', 'kamu', 'berlaku', 'ad', '##il', 'dan', 'be', '##rb', '##uat', 'ke', '##ba', '##jikan', '.']\n",
      "Token IDs: [  101 23387 10716 12589 55093 22734 10588 25285 52794 90618 53347 10840\n",
      " 11030 10215 10347 50579 48439 11163 10537 97374   119   102]\n",
      "Attention Mask: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Text: Janganlah sekali-kali kebencianmu terhadap suatu kaum mendorong kamu untuk berlaku tidak adil.\n",
      "Tokens: ['Jang', '##an', '##lah', 'sekali', '-', 'kali', 'ke', '##ben', '##cian', '##mu', 'terhadap', 'suatu', 'kaum', 'men', '##doro', '##ng', 'kamu', 'untuk', 'berlaku', 'tidak', 'ad', '##il', '.']\n",
      "Token IDs: [  101 65034 10206 16254 46233   118 16384 11163 10965 35288 11717 21420\n",
      " 22370 23121 10588 79517 10376 90618 10782 53347 11868 10840 11030   119\n",
      "   102]\n",
      "Attention Mask: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Text: Allah tidak menyukai orang yang berbuat zalim.\n",
      "Tokens: ['Allah', 'tidak', 'men', '##yu', '##kai', 'orang', 'yang', 'be', '##rb', '##uat', 'zal', '##im', '.']\n",
      "Token IDs: [  101 22734 11868 10588 25285 18511 12430 10265 10347 50579 48439 30639\n",
      " 11759   119   102]\n",
      "Attention Mask: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_embedding = get_embedding(query, tokenizer, model)\n",
    "ayat_embeddings = [get_embedding(ayat, tokenizer, model) for ayat in ayat_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: indolem/indobert-base-uncased\n",
      "Tokens: ['mengerjakan', 'pekerjaan', 'dengan', 'adil', 'dan', 'bijaksana']\n",
      "Token IDs: [8639, 3543, 1545, 6187, 1501, 11387]\n",
      "Jumlah Token: 6\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: bert-base-multilingual-cased\n",
      "Tokens: ['men', '##ger', '##jakan', 'pekerjaan', 'dengan', 'ad', '##il', 'dan', 'bija', '##ksa', '##na']\n",
      "Token IDs: [10588, 11446, 103497, 101362, 10659, 10840, 11030, 10215, 14832, 42430, 10219]\n",
      "Jumlah Token: 11\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: xlm-roberta-base\n",
      "Tokens: ['▁mengerjakan', '▁pekerjaan', '▁dengan', '▁adil', '▁dan', '▁bija', 'ksana']\n",
      "Token IDs: [156477, 40749, 462, 130916, 123, 6845, 169828]\n",
      "Jumlah Token: 7\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model: indobenchmark/indobert-base-p2\n",
      "Tokens: ['mengerjakan', 'pekerjaan', 'dengan', 'adil', 'dan', 'bijaksana']\n",
      "Token IDs: [5086, 1367, 79, 5754, 41, 10249]\n",
      "Jumlah Token: 6\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_tokenization(models, text):\n",
    "    for model_name in models:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        print(f\"Token IDs: {token_ids}\")\n",
    "        print(f\"Jumlah Token: {len(tokens)}\")\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Daftar model yang ingin diuji\n",
    "model_names = [\n",
    "    \"indolem/indobert-base-uncased\",\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    \"xlm-roberta-base\",\n",
    "    \"indobenchmark/indobert-base-p2\",\n",
    "    # \"indobenchmark/indobart\",\n",
    "    # \"indobenchmark/indogpt\",\n",
    "\n",
    "]\n",
    "\n",
    "text = \"mengerjakan pekerjaan dengan adil dan bijaksana\"\n",
    "# text = \"doing work fairly and wisely\"\n",
    "evaluate_tokenization(model_names, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,  2000,  4637,  2007,  8407,  2655,  1016,     2,     1],\n",
      "        [    0,  2013,  1009,  1059,  2065, 11563,  2652,  1003,     2],\n",
      "        [    0,  2006,  5229,  2004,  2000,  3350,  1016,     2,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained SentenceTransformer model\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# Encode some texts\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]\n",
    "embeddings = model.tokenize(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks Asli: mengerjakan pekerjaan dengan adil dan bijaksana\n",
      "Tokens: ['meng', '##er', '##jak', '##an', 'pe', '##ker', '##ja', '##an', 'deng', '##an', 'adi', '##l', 'dan', 'bi', '##jak', '##san', '##a']\n",
      "Token IDs: [    0 27959  2125 18321  2323 21881  5488  3904  2323 26961  2323 27137\n",
      "  2144  4911 12174 18321  8795  2054     2]\n",
      "Attention Mask: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "Embeddings shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Nama model SBERT\n",
    "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "# Inisialisasi tokenizer dan model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Teks yang ingin di-tokenisasi\n",
    "text = \"mengerjakan pekerjaan dengan adil dan bijaksana\"\n",
    "\n",
    "# Tokenisasi teks\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "tokens = tokenizer.tokenize(text)  # Mendapatkan token sebagai teks\n",
    "token_ids = inputs['input_ids'][0].numpy()  # Mendapatkan token IDs dalam format numpy array\n",
    "\n",
    "# Menampilkan detail tokenisasi\n",
    "print(f\"Teks Asli: {text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Attention Mask: {inputs['attention_mask'][0].numpy()}\")\n",
    "\n",
    "# Jika ingin mendapatkan embedding dari model\n",
    "with torch.no_grad():\n",
    "    embeddings = model.encode(text)\n",
    "\n",
    "print(\"\\nEmbeddings shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ayat: Sesungguhnya Allah menyuruh kamu berlaku adil dan berbuat kebajikan.\n",
      "Score: 0.4979539215564728\n",
      "\n",
      "Ayat: Janganlah sekali-kali kebencianmu terhadap suatu kaum mendorong kamu untuk berlaku tidak adil.\n",
      "Score: 0.4779987633228302\n",
      "\n",
      "Ayat: Allah tidak menyukai orang yang berbuat zalim.\n",
      "Score: 0.38167575001716614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarity\n",
    "similarities = [cosine_similarity(query_embedding, ayat_embedding)[0][0] for ayat_embedding in ayat_embeddings]\n",
    "\n",
    "# Rank ayat by similarity\n",
    "ranking = sorted(zip(ayat_list, similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print ranked ayat\n",
    "for ayat, score in ranking:\n",
    "    print(f\"Ayat: {ayat}\\nScore: {score}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Dengan nama Allah Yang Maha Pengasih, Maha Pen...\n",
       "1          Segala puji bagi Allah, Tuhan seluruh alam,\n",
       "2                  Yang Maha Pengasih, Maha Penyayang,\n",
       "3                             Pemilik hari pembalasan.\n",
       "4    Hanya kepada Engkaulah kami menyembah dan hany...\n",
       "5                    Tunjukilah kami jalan yang lurus,\n",
       "6    (yaitu) jalan orang-orang yang telah Engkau be...\n",
       "Name: indoText, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('quran_100_ayat.csv')\n",
    "ayat_list = df['indoText'].head(7)\n",
    "ayat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'kasih sayang'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: kasih sayang\n",
      "Tokens: ['kasih', 'sayang']\n",
      "Token IDs: [   3 3774 5458    4]\n",
      "Attention Mask: [1 1 1 1]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Text: Dengan nama Allah Yang Maha Pengasih, Maha Penyayang.\n",
      "Tokens: ['dengan', 'nama', 'allah', 'yang', 'maha', 'pengasih', ',', 'maha', 'penyayang', '.']\n",
      "Token IDs: [    3  1545  1966  2211  1497  5774 24570    16  5774 21331    18     4]\n",
      "Attention Mask: [1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Text: Segala puji bagi Allah, Tuhan seluruh alam,\n",
      "Tokens: ['segala', 'puji', 'bagi', 'allah', ',', 'tuhan', 'seluruh', 'alam', ',']\n",
      "Token IDs: [    3  3157 11832  1896  2211    16  2702  2313  2660    16     4]\n",
      "Attention Mask: [1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Text: Yang Maha Pengasih, Maha Penyayang,\n",
      "Tokens: ['yang', 'maha', 'pengasih', ',', 'maha', 'penyayang', ',']\n",
      "Token IDs: [    3  1497  5774 24570    16  5774 21331    16     4]\n",
      "Attention Mask: [1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Text: Pemilik hari pembalasan.\n",
      "Tokens: ['pemilik', 'hari', 'pembalasan', '.']\n",
      "Token IDs: [    3  4199  1843 20360    18     4]\n",
      "Attention Mask: [1 1 1 1 1 1]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Text: Hanya kepada Engkaulah kami menyembah dan hanya kepada Engkaulah kami mohon pertolongan.\n",
      "Tokens: ['hanya', 'kepada', 'engkau', '##lah', 'kami', 'menyembah', 'dan', 'hanya', 'kepada', 'engkau', '##lah', 'kami', 'mohon', 'pertolongan', '.']\n",
      "Token IDs: [    3  1821  1767  4945  1656  2087 13207  1501  1821  1767  4945  1656\n",
      "  2087  9153 10558    18     4]\n",
      "Attention Mask: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Text: Tunjukilah kami jalan yang lurus,\n",
      "Tokens: ['tunjuk', '##ilah', 'kami', 'jalan', 'yang', 'lurus', ',']\n",
      "Token IDs: [    3 30494  2404  2087  2050  1497  9055    16     4]\n",
      "Attention Mask: [1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Text: (yaitu) jalan orang-orang yang telah Engkau beri nikmat kepadanya; bukan (jalan) mereka yang dimurkai, dan bukan (pula jalan) mereka yang sesat.\n",
      "Tokens: ['(', 'yaitu', ')', 'jalan', 'orang', '-', 'orang', 'yang', 'telah', 'engkau', 'beri', 'nikmat', 'kepadanya', ';', 'bukan', '(', 'jalan', ')', 'mereka', 'yang', 'dim', '##ur', '##kai', ',', 'dan', 'bukan', '(', 'pula', 'jalan', ')', 'mereka', 'yang', 'sesat', '.']\n",
      "Token IDs: [    3    12  2125    13  2050  1646    17  1646  1497  1703  4945  8448\n",
      " 11477  5872    31  2054    12  2050    13  1633  1497  1887  1514 19793\n",
      "    16  1501  2054    12  2547  2050    13  1633  1497 10522    18     4]\n",
      "Attention Mask: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get embeddings for query and ayat\n",
    "query_embedding = get_embedding(query)\n",
    "ayat_embeddings = [get_embedding(ayat) for ayat in ayat_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ayat: Yang Maha Pengasih, Maha Penyayang,\n",
      "Score: 0.12875959277153015\n",
      "\n",
      "Ayat: Pemilik hari pembalasan.\n",
      "Score: 0.12072833627462387\n",
      "\n",
      "Ayat: Dengan nama Allah Yang Maha Pengasih, Maha Penyayang.\n",
      "Score: 0.11339651048183441\n",
      "\n",
      "Ayat: Segala puji bagi Allah, Tuhan seluruh alam,\n",
      "Score: 0.07025673985481262\n",
      "\n",
      "Ayat: Tunjukilah kami jalan yang lurus,\n",
      "Score: 0.06887882947921753\n",
      "\n",
      "Ayat: (yaitu) jalan orang-orang yang telah Engkau beri nikmat kepadanya; bukan (jalan) mereka yang dimurkai, dan bukan (pula jalan) mereka yang sesat.\n",
      "Score: 0.04936020076274872\n",
      "\n",
      "Ayat: Hanya kepada Engkaulah kami menyembah dan hanya kepada Engkaulah kami mohon pertolongan.\n",
      "Score: 0.03851040452718735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarity\n",
    "similarities = [cosine_similarity(query_embedding, ayat_embedding)[0][0] for ayat_embedding in ayat_embeddings]\n",
    "\n",
    "# Rank ayat by similarity\n",
    "ranking = sorted(zip(ayat_list, similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print ranked ayat\n",
    "for ayat, score in ranking:\n",
    "    print(f\"Ayat: {ayat}\\nScore: {score}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
