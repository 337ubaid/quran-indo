{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script contains an example how to extend an existent sentence embedding model to new languages.\n",
    "\n",
    "Given a (monolingual) teacher model you would like to extend to new languages, which is specified in the teacher_model_name\n",
    "variable. We train a multilingual student model to imitate the teacher model (variable student_model_name)\n",
    "on multiple languages.\n",
    "\n",
    "For training, you need parallel sentence data (machine translation training data). You need tab-seperated files (.tsv)\n",
    "with the first column a sentence in a language understood by the teacher model, e.g. English,\n",
    "and the further columns contain the according translations for languages you want to extend to.\n",
    "\n",
    "This scripts downloads automatically the parallel sentences corpus. This corpus contains transcripts from\n",
    "talks translated to 100+ languages. For other parallel data, see get_parallel_data_[].py scripts\n",
    "\n",
    "Further information can be found in our paper:\n",
    "Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation\n",
    "https://arxiv.org/abs/2004.09813\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "from sentence_transformers import LoggingHandler, SentenceTransformer\n",
    "from sentence_transformers.evaluation import (\n",
    "    EmbeddingSimilarityEvaluator,\n",
    "    MSEEvaluator,\n",
    "    SequentialEvaluator,\n",
    "    TranslationEvaluator,\n",
    ")\n",
    "from sentence_transformers.losses import MSELoss\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO, handlers=[LoggingHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# The teacher model is monolingual, we use it for English embeddings\n",
    "teacher_model_name = \"paraphrase-distilroberta-base-v2\"\n",
    "# The student model is multilingual, we train it such that embeddings of non-English texts mimic the teacher model's English embeddings\n",
    "student_model_name = \"xlm-roberta-base\"\n",
    "\n",
    "student_max_seq_length = 128  # Student model max. lengths for inputs (number of word pieces)\n",
    "train_batch_size = 8  # Batch size for training\n",
    "inference_batch_size = 32  # Batch size at inference\n",
    "max_sentences_per_language = 500000  # Maximum number of  parallel sentences for training\n",
    "\n",
    "num_train_epochs = 5  # Train for x epochs\n",
    "num_evaluation_steps = 5000  # Evaluate performance after every xxxx steps\n",
    "\n",
    "\n",
    "# Define the language codes you would like to extend the model to\n",
    "source_languages = set([\"en\"])  # Our teacher model accepts English (en) sentences\n",
    "# We want to extend the model to these new languages. For language codes, see the header of the train file\n",
    "target_languages = set([\"id\"])\n",
    "\n",
    "\n",
    "output_dir = (\n",
    "    \"output/make-multilingual-\"\n",
    "    + \"-\".join(sorted(list(source_languages)) + sorted(list(target_languages)))\n",
    "    + \"-\"\n",
    "    + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en'}\n",
      "{'id'}\n"
     ]
    }
   ],
   "source": [
    "print(source_languages)\n",
    "print(target_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-23 14:34:00 - Use pytorch device_name: cuda\n",
      "2025-03-23 14:34:00 - Load pretrained SentenceTransformer: paraphrase-distilroberta-base-v2\n",
      "2025-03-23 14:34:07 - Teacher model: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "2025-03-23 14:34:07 - Use pytorch device_name: cuda\n",
      "2025-03-23 14:34:07 - Load pretrained SentenceTransformer: xlm-roberta-base\n",
      "2025-03-23 14:34:08 - No sentence-transformers model found with name xlm-roberta-base. Creating a new one with mean pooling.\n",
      "2025-03-23 14:34:13 - Student model: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 1a. Here we define our SentenceTransformer teacher model.\n",
    "teacher_model = SentenceTransformer(teacher_model_name)\n",
    "# If we want, we can limit the maximum sequence length for the model\n",
    "# teacher_model.max_seq_length = 128\n",
    "logging.info(f\"Teacher model: {teacher_model}\")\n",
    "\n",
    "# 1b. Here we define our SentenceTransformer student model. If not already a Sentence Transformer model,\n",
    "# it will automatically create one with \"mean\" pooling.\n",
    "student_model = SentenceTransformer(student_model_name)\n",
    "# If we want, we can limit the maximum sequence length for the model\n",
    "student_model.max_seq_length = student_max_seq_length\n",
    "logging.info(f\"Student model: {student_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-23 14:34:23 - Could not load dataset multi_lang_dataset.csv/en-id dev split, splitting 1k samples from train\n",
      "2025-03-23 14:34:23 - DatasetDict({\n",
      "    en-id: Dataset({\n",
      "        features: ['english', 'non_english'],\n",
      "        num_rows: 5236\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c991758febd84bf0ab1fc0f3caedbe2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5236 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-23 14:34:46 - Prepared datasets for training:\n"
     ]
    }
   ],
   "source": [
    "# 2. Load the parallel sentences training dataset: https://huggingface.co/datasets?other=sentence-transformers&sort=trending&search=parallel-sentences\n",
    "# NOTE: We can also use multiple datasets if we want\n",
    "dataset_to_use = \"multi_lang_dataset.csv\"\n",
    "# dataset_to_use = \"sentence-transformers/parallel-sentences-talks\"\n",
    "# dataset_to_use = \"sentence-transformers/parallel-sentences-europarl\"\n",
    "# dataset_to_use = \"sentence-transformers/parallel-sentences-global-voices\"\n",
    "# dataset_to_use = \"sentence-transformers/parallel-sentences-muse\"\n",
    "# dataset_to_use = \"sentence-transformers/parallel-sentences-jw300\"\n",
    "# dataset_to_use = \"sentence-transformers/parallel-sentences-news-commentary\"\n",
    "# dataset_to_use = \"sentence-transformers/parallel-sentences-opensubtitles\"\n",
    "# dataset_to_use = \"sentence-transformers/parallel-sentences-tatoeba\"\n",
    "# dataset_to_use = \"sentence-transformers/parallel-sentences-wikimatrix\"\n",
    "# dataset_to_use = \"sentence-transformers/parallel-sentences-wikititles\"\n",
    "train_dataset_dict = DatasetDict()\n",
    "eval_dataset_dict = DatasetDict()\n",
    "for source_lang in source_languages:\n",
    "    for target_lang in target_languages:\n",
    "        subset = f\"{source_lang}-{target_lang}\"\n",
    "        try:\n",
    "            train_dataset = load_dataset('csv', subset, data_files=dataset_to_use, split=\"train\")\n",
    "            if len(train_dataset) > max_sentences_per_language:\n",
    "                train_dataset = train_dataset.select(range(max_sentences_per_language))\n",
    "        except Exception as exc:\n",
    "            logging.error(f\"Could not load dataset {dataset_to_use}/{source_lang}-{target_lang}: {exc}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            eval_dataset = load_dataset(dataset_to_use, subset, split=\"dev\")\n",
    "            if len(eval_dataset) > 1000:\n",
    "                eval_dataset = eval_dataset.select(range(1000))\n",
    "        except Exception:\n",
    "            logging.info(\n",
    "                f\"Could not load dataset {dataset_to_use}/{source_lang}-{target_lang} dev split, splitting 1k samples from train\"\n",
    "            )\n",
    "            dataset = train_dataset.train_test_split(test_size=1000, shuffle=True)\n",
    "            train_dataset = dataset[\"train\"]\n",
    "            eval_dataset = dataset[\"test\"]\n",
    "\n",
    "        train_dataset_dict[subset] = train_dataset\n",
    "        eval_dataset_dict[subset] = eval_dataset\n",
    "logging.info(train_dataset_dict)\n",
    "\n",
    "\n",
    "# We want the student EN embeddings to be similar to the teacher EN embeddings and\n",
    "# the student non-EN embeddings to be similar to the teacher EN embeddings\n",
    "def prepare_dataset(batch):\n",
    "    return {\n",
    "        \"english\": batch[\"english\"],\n",
    "        \"non_english\": batch[\"non_english\"],\n",
    "        \"label\": teacher_model.encode(batch[\"english\"], batch_size=inference_batch_size, show_progress_bar=False),\n",
    "    }\n",
    "\n",
    "\n",
    "column_names = list(train_dataset_dict.values())[0].column_names\n",
    "train_dataset_dict = train_dataset_dict.map(\n",
    "    prepare_dataset, batched=True, batch_size=30000, remove_columns=column_names\n",
    ")\n",
    "logging.info(\"Prepared datasets for training:\", train_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define our training loss\n",
    "# MSELoss (https://sbert.net/docs/package_reference/sentence_transformer/losses.html#mseloss) needs one text columns and one\n",
    "# column with embeddings from the teacher model\n",
    "train_loss = MSELoss(model=student_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-23 14:35:01 - Creating evaluators for en-id\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2049e3f57b40af833c7b7feff16ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 4. Define evaluators for use during training. This is useful to keep track of alongside the evaluation loss.\n",
    "evaluators = []\n",
    "\n",
    "for subset, eval_dataset in eval_dataset_dict.items():\n",
    "    logger.info(f\"Creating evaluators for {subset}\")\n",
    "\n",
    "    # Mean Squared Error (MSE) measures the (euclidean) distance between teacher and student embeddings\n",
    "    dev_mse = MSEEvaluator(\n",
    "        source_sentences=eval_dataset[\"english\"],\n",
    "        target_sentences=eval_dataset[\"non_english\"],\n",
    "        name=subset,\n",
    "        teacher_model=teacher_model,\n",
    "        batch_size=inference_batch_size,\n",
    "    )\n",
    "    evaluators.append(dev_mse)\n",
    "\n",
    "    # TranslationEvaluator computes the embeddings for all parallel sentences. It then check if the embedding of\n",
    "    # source[i] is the closest to target[i] out of all available target sentences\n",
    "    dev_trans_acc = TranslationEvaluator(\n",
    "        source_sentences=eval_dataset[\"english\"],\n",
    "        target_sentences=eval_dataset[\"non_english\"],\n",
    "        name=subset,\n",
    "        batch_size=inference_batch_size,\n",
    "    )\n",
    "    evaluators.append(dev_trans_acc)\n",
    "\n",
    "    # Try to load this subset from STS17\n",
    "    test_dataset = None\n",
    "    try:\n",
    "        test_dataset = load_dataset(\"mteb/sts17-crosslingual-sts\", subset, split=\"test\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            test_dataset = load_dataset(\"mteb/sts17-crosslingual-sts\", f\"{subset[3:]}-{subset[:2]}\", split=\"test\")\n",
    "            subset = f\"{subset[3:]}-{subset[:2]}\"\n",
    "        except Exception:\n",
    "            pass\n",
    "    if test_dataset:\n",
    "        test_evaluator = EmbeddingSimilarityEvaluator(\n",
    "            sentences1=test_dataset[\"sentence1\"],\n",
    "            sentences2=test_dataset[\"sentence2\"],\n",
    "            scores=[score / 5.0 for score in test_dataset[\"score\"]],  # Convert 0-5 scores to 0-1 scores\n",
    "            batch_size=inference_batch_size,\n",
    "            name=f\"sts17-{subset}-test\",\n",
    "            show_progress_bar=False,\n",
    "        )\n",
    "        evaluators.append(test_evaluator)\n",
    "\n",
    "evaluator = SequentialEvaluator(evaluators, main_score_function=lambda scores: np.mean(scores))\n",
    "# Now also prepare the evaluation datasets for training\n",
    "eval_dataset_dict = eval_dataset_dict.map(prepare_dataset, batched=True, batch_size=500, remove_columns=column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Define the training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=output_dir,\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=train_batch_size,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "    learning_rate=2e-5,\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=num_evaluation_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=num_evaluation_steps,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    run_name=f\"multilingual-{'-'.join(source_languages)}-{'-'.join(target_languages)}\",  # Will be used in W&B if `wandb` is installed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08b7e4a8e5c439282cc409fee9f2de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='820' max='820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [820/820 1:41:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-19 14:53:01 - Saving model checkpoint to output/make-multilingual-en-id-2025-03-19_13-10-48\\checkpoint-820\n",
      "2025-03-19 14:53:01 - Save model to output/make-multilingual-en-id-2025-03-19_13-10-48\\checkpoint-820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=820, training_loss=0.2218638202039207, metrics={'train_runtime': 6090.7567, 'train_samples_per_second': 4.298, 'train_steps_per_second': 0.135, 'total_flos': 0.0, 'train_loss': 0.2218638202039207, 'epoch': 5.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Create the trainer & start training\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=student_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset_dict,\n",
    "    eval_dataset=eval_dataset_dict,\n",
    "    loss=train_loss,\n",
    "    evaluator=evaluator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-19 14:53:08 - Save model to output/make-multilingual-en-id-2025-03-19_13-10-48/final\n"
     ]
    }
   ],
   "source": [
    "# 7. Save the trained & evaluated model locally\n",
    "final_output_dir = f\"{output_dir}/final\"\n",
    "student_model.save(final_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. (Optional) save the model to the Hugging Face Hub!\n",
    "# It is recommended to run `huggingface-cli login` to log into your Hugging Face account first\n",
    "model_name = student_model_name if \"/\" not in student_model_name else student_model_name.split(\"/\")[-1]\n",
    "try:\n",
    "    student_model.push_to_hub(f\"{model_name}-multilingual-{'-'.join(source_languages)}-{'-'.join(target_languages)}\")\n",
    "except Exception:\n",
    "    logging.error(\n",
    "        f\"Error uploading model to the Hugging Face Hub:\\n{traceback.format_exc()}To upload it manually, you can run \"\n",
    "        f\"`huggingface-cli login`, followed by loading the model using `model = SentenceTransformer({final_output_dir!r})` \"\n",
    "        f\"and saving it using `model.push_to_hub('{model_name}-multilingual-{'-'.join(source_languages)}-{'-'.join(target_languages)}')`.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_dir = 'output/make-multilingual-en-id-2025-03-19_13-10-48/final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaModel(\n",
      "  (embeddings): XLMRobertaEmbeddings(\n",
      "    (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (token_type_embeddings): Embedding(1, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): XLMRobertaEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x XLMRobertaLayer(\n",
      "        (attention): XLMRobertaAttention(\n",
      "          (self): XLMRobertaSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): XLMRobertaSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): XLMRobertaIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): XLMRobertaOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): XLMRobertaPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name_or_path = final_output_dir\n",
    "model = AutoModel.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Cek apakah model berhasil dimuat\n",
    "print(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
